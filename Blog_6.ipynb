{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blog-6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPg9PPA5hrdtpk7rGi3PpF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SapanaChaudhary/Colab-pages/blob/master/Blog_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fDeZcRk2W3",
        "colab_type": "text"
      },
      "source": [
        "This blog post is an opus of my naive understanding of various ideas from Asymptotic Statistics. The ideas are listed here (in no particular order): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4takp2nltkf",
        "colab_type": "text"
      },
      "source": [
        "[Hypothesis testing - a gist](https://colab.research.google.com/drive/1fGRLnPPqTyfLiXBd3p2q7AEXzbueaRZC#scrollTo=Rkun8Ct_mItW&line=1&uniqifier=1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LptBGI03mDxf",
        "colab_type": "text"
      },
      "source": [
        "[(1-$\\alpha$) confidence interval]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ntNvTJOngb5",
        "colab_type": "text"
      },
      "source": [
        "[On consistency and aymptotic normality]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZLAcmmJnwnx",
        "colab_type": "text"
      },
      "source": [
        "[Going from convergence in distribution to moments]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkun8Ct_mItW",
        "colab_type": "text"
      },
      "source": [
        "## Hypothesis testing - a gist \n",
        "\n",
        "Performing hypothesis testing can be summarized in the following five steps: \n",
        "1. Define $H_0, H_a$. \n",
        "1. Set significance level $\\alpha$.\n",
        "1. Take sample (n). Find $\\bar{x}$, $\\bar{s}$. \n",
        "1. $p$-value: $P(\\bar{x}$ is something $|H_0$ true).\n",
        "1. $p$-value < $\\alpha \\implies$ Reject $H_0$ (_i.e._ We have evidence for $H_a$).\n",
        "   \n",
        "   $p$-value > $\\alpha \\implies$ Do not reject $H_0$. \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sUJ6S1JoVKf",
        "colab_type": "text"
      },
      "source": [
        "## (1-$\\alpha$) confidence interval\n",
        "\n",
        "Given the distribution of sample means, a confidence interval means - you are confident that the sample mean lies in (1-$\\alpha$) range. \n",
        "\n",
        "![Confidence interval](https://lh3.googleusercontent.com/-jj7Em1Bkuxc/XoU4AyxGs5I/AAAAAAAAGgw/mt2fHuEUiQA3Ibwwr4VPPfsvlIfJoUYowCK8BGAsYHg/s0/confidence_intterval_1.png)\n",
        "\n",
        "Assume we know the sample mean for 'n' number of samples and the population variance = $\\sigma^2$. Then, the confidence interval for mean can be defined as: \n",
        "\n",
        "$\\Big[ \\bar{x} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\Big]$, \n",
        "where $z_{\\alpha/2}$ is $\\alpha$ level's $z$-score for a two tailed test such that $P(|Z|\\leq z_{\\alpha/2}) = 1-\\alpha$. \n",
        "\n",
        "\n",
        "Why do we multiply std error with z-score?\n",
        "To quantify 'how much std dev you are away from the mean?'. \n",
        "\n",
        "When $\\sigma$ is unknown, and n is large, $\\sigma$ can be replaced by its estimator.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zk6vXpGKN1H",
        "colab_type": "text"
      },
      "source": [
        "## On consistency and aysmptotic normality \n",
        "\n",
        "Given an estimator $\\hat{Q}$ for a model $M$ (like non-linear regression). \n",
        "\n",
        "If M = Linear, analytical solution for $\\hat{Q}$ can be obtained. \n",
        "If M = non-linear, analytical solution might not exist. \n",
        "\n",
        "To study consistency of $\\hat{Q}$, we need to know the asymptotic behaviour - probability or almost sure limit behaviour - of $\\hat{Q}$ on the set of parameters. This is easier to do when an analytical form for $\\hat{Q}$ exists. \n",
        "\n",
        "When no analytical form for $\\hat{Q}$ exists, we need to resort to \"uniform law of large numbers (or empirical process theory)\" to obtain the asymptotic behaviour of $\\hat{Q}$. \n",
        "\n",
        "\n",
        "To study asymptotic normality, we need to expand the objective function (of the model?) around the true value of parameters (true value is obtained using consistency analysis). The expansion aims to obtain the aymptotic dispersion of $\\hat{Q}$ around the true value, namely the asymptotic distribution of the estimators. The expansion is achieved using Mean Value Theorem. "
      ]
    }
  ]
}